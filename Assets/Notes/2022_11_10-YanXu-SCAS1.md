# SCAS系统

## 项目介绍

利用**人脸识别技术**针对不同机位的视频数据，有效识别出用户出现的时间段，并利用**视频剪辑算法**将各机位视频片段进行整合，形成用户专属的精彩短视频。

局域网方案流程，如下图所示：

![img](.\imgs\2022_11_10-YanXu-SCAS1.assets\wps1.jpg) 

局域网方案流程图

1）用户基于门口的广告机和摄像头拍摄自己的照片并上传；

2）管理人员通过开始控制模块启动整个SCAS分析处理，机器人和固定机位开始摄像并实时推流；

3）接流服务器接收视频数据，人脸识别推理模型开始逐帧分析；

4）用户参观完，管理人员通过结束控制模块终止推流，人脸识别推理模块处理完所有视频，开始智能选取精彩画面，并添加片头片尾、配音配乐等；

5）完成的视频传输到展示平台，在大屏上展示。

## **代码路径**

算法代码路径：/home/scas_cv/Documents/SCAS1/algorithm

后端代码路径：/home/scas_cv/Documents/SCAS1/Backend

后端运行：python manage.py runserver 0.0.0.0:8000

算法运行：worker6.py和monitor.py

http://117.32.250.98:8026/#/login 外网访问地址

## **项目执行流程**

**节目开始前：**

- **(删除文件：)** 

- -  **/home/scas_cv/Documents/SCAS1/algorithm/delete_file.py**(删除上次节目的一些视频文件等)

  -  **配置nginx**

**上传照片：**白喜文、韩柯、胡冠宇

**上传路径**：/home/scas_cv/Documents/SCAS1/Backend/scas_be/media/img

**开始系统**

**节目开始后：**

- **模拟推流** cpfile_1.py 或 cpfile.py，**直接用cpfile.py就行**

- - **推流的视频来自：**/home/scas_cv/Test_Videos_xxx

  -   1 2 0618containactor 是一天的素材 可用韩柯或者胡冠宇师兄的照片上传

  -   0813 用不太上 

  -   0819multiactor和 3456lu 是在实验室拍摄的多人素材 

**推流视频的保存路径：**/home/scas_cv/Videos

**节目进行期间：**

- **手动上传视频：**

- -  子弹时间视频

  -  虚拟演播室视频

  -  **判断是否满足实时性：**  

  -  **执行代码：**/home/scas_cv/Documents/SCAS1/algorithm/print_num_videoandpkl.py

  -  **原理：**比较推的视频数量和分析出来的pkl数量差值

  -  **实时查看显卡占用情况：**

  -  终端命令：watch -n 0.5 nvidia-smi 

## **分布式进程**

通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue。

Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。

引用自https://www.jianshu.com/p/9503c7b1d538

​	首先编写个manager服务器

```python
# encoding:utf-8 
import random, time, Queue 
from multiprocessing.managers import BaseManager 
# 发送任务的队列 
task_queue = Queue.Queue() 
# 接收结果的队列 result_queue = Queue.Queue() 
# 使用标准函数来代替lambda函数，避免python2.7中，pickle无法序列化lambda的问题 
def get_task_queue(): 
    global task_queue 
    return task_queue 
# 使用标准函数来代替lambda函数，避免python2.7中，pickle无法序列化lambda的问题 
def get_result_queue(): 
    global task_queue 
    return task_queue 
def startManager(host, port, authkey): 
    # 把两个Queue都注册到网络上，callable参数关联了Queue对象，注意回调函数不能使用括号 
    BaseManager.register('get_result_queue', callable=get_result_queue) 
    # 设置host,绑定端口port，设置验证码为authkey 
    manager = BaseManager(address=(host, port), authkey=authkey) 
    # 启动manager服务器 
    manager.start() 
    return manager 
def put_queue(manager): 
    # 通过网络访问
    queueu_task = manager.get_task_queue() 
    while 1: 
        n = random.randint(0, 1000) 
        print ('Put task %d' % n) 
        task.put(n) 
        time.sleep(0.5) 


if __name__ == "__main__": 
    host = '127.0.0.1' 
    port = 5000 
    authkey = 'abc' 
    # 启动manager服务器 
    manager = startManager(host, port, authkey) 
    # 给task队列添加数据 
    put_queue(manager) 
    # 关闭服务器 
    manager.shutdown
```

然后编写worker

```python
# encoding:utf-8

import random, time, Queue
from multiprocessing.managers import BaseManager


def start_worker(host, port, authkey):
    # 由于这个BaseManager只从网络上获取queue，所以注册时只提供名字
    BaseManager.register('get_task_queue')
    BaseManager.register('get_result_queue')
    print ('Connect to server %s' % host)
    # 注意，端口port和验证码authkey必须和manager服务器设置的完全一致
    worker = BaseManager(address=(host, port), authkey=authkey)
    # 链接到manager服务器
    worker.connect()
    return worker


def get_queue(worker):
    task = worker.get_task_queue()
    result = worker.get_result_queue()
    # 从task队列取数据，并添加到result队列中
    while 1:
        if task.empty():
            time.sleep(1)
            continue
        n = task.get(timeout=1)
        print ('worker get %d' % n)
        result.put(n)
        time.sleep(1)

if __name__ == "__main__":
    host = '127.0.0.1'
    port = 5000
    authkey = 'abc'
    # 启动worker
    worker = start_worker(host, port, authkey)
    # 获取队列
    get_queue(worker)
```

启动一个manager和两个worker，worker1+worker2的数据了等于manager服务器的数据，并且没有重复的值

